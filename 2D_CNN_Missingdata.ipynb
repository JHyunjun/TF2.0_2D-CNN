{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "private_outputs": true,
      "provenance": [],
      "gpuType": "T4",
      "mount_file_id": "1jiZsjgYXqGbSKjoDFqjVp7VjEteJuEE_",
      "authorship_tag": "ABX9TyPMkELsUiD/EEQAI8FGexKh",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/JHyunjun/TF2.0_2D-CNN/blob/main/2D_CNN_Missingdata.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import numpy as np\n",
        "\n",
        "xy_train = np.loadtxt('/content/drive/MyDrive/Colab Notebooks/LSTM/train_v.txt',delimiter=',',dtype=np.float32) #5808\n",
        "xy_test = np.loadtxt('/content/drive/MyDrive/Colab Notebooks/LSTM/test_v.txt',delimiter=',',dtype=np.float32) #10883\n",
        "xy_train = np.delete(xy_train ,3, axis = 1)\n",
        "xy_test = np.delete(xy_test ,3, axis = 1)\n",
        "\n",
        "start_test = 1000\n",
        "middle_test = 2000\n",
        "\n",
        "middle_test1 = 3000\n",
        "end_test = 10000\n",
        "\n",
        "xy_test_1 = xy_test[:start_test,:]\n",
        "xy_test_1n2 = xy_test[start_test:middle_test,:]\n",
        "xy_test_2 = xy_test[middle_test1:end_test,:]\n",
        "\n",
        "xy_train = np.concatenate((xy_train, xy_test_1, xy_test_2), axis = 0)\n",
        "xy_train_Y = np.copy(xy_train)\n",
        "#xy_test = np.copy(xy_train) # For Adding AWGN\n",
        "xy_test = xy_test_1n2\n",
        "xy_test_Y = np.copy(xy_test)\n",
        "\n",
        "#Normalization\n",
        "\n",
        "def MinMaxScaler(data) : # Normal Std\n",
        "  numerator = data - np.min(data,0)\n",
        "  denominator = np.max(data,0) - np.min(data,0)\n",
        "\n",
        "  return numerator / (denominator + 1e-7)\n",
        "\n",
        "def origin_minmax(data) : # HJ STYLE\n",
        "  arr_max = np.zeros(data.shape[1])\n",
        "  arr_min = np.zeros(data.shape[1])\n",
        "\n",
        "  for i in range(data.shape[1]) :\n",
        "    max = np.max(data[:,i])\n",
        "    min = np.min(data[:,i])\n",
        "    arr_max[i] = max\n",
        "    arr_min[i] = min\n",
        "    pass\n",
        "  return arr_max, arr_min\n",
        "\n",
        "arr_max_train, arr_min_train = origin_minmax(xy_train)\n",
        "arr_max_test, arr_min_test = origin_minmax(xy_test)\n",
        "arr_max_total = arr_max_train\n",
        "arr_min_total = arr_min_train\n",
        "print(arr_max_train, arr_min_train)\n",
        "print(arr_max_test, arr_min_test)\n",
        "\n",
        "for i in range(xy_train.shape[1]) :\n",
        "  if(arr_max_train[i] > arr_max_test[i]) :\n",
        "    arr_max_total[i] = arr_max_train[i]\n",
        "  else :\n",
        "    arr_max_total[i] = arr_max_test[i]\n",
        "  pass\n",
        "\n",
        "for i in range(xy_train.shape[1]) :\n",
        "  if(arr_min_train[i] < arr_min_test[i]) :\n",
        "    arr_min_total[i] = arr_min_train[i]\n",
        "  else :\n",
        "    arr_min_total[i] = arr_min_test[i]\n",
        "  pass\n",
        "\n",
        "print(arr_max_total, arr_min_total)\n",
        "\n",
        "def HJ_MinMaxScaler(data) :\n",
        "  for i in range(data.shape[1]) :\n",
        "    numerator = data[:,i] - arr_min_total[i]\n",
        "    denominator = arr_max_total[i] - arr_min_total[i]\n",
        "    data[:,i] = numerator / (denominator + 1e-7)\n",
        "\n",
        "  return data\n",
        "\n",
        "def HJ_backMinMax(data) :\n",
        "  for i in range(data.shape[1]) :\n",
        "    data[:,i] = data[:,i] * (arr_max_total[i] - arr_min_total[i]) + arr_min_total[i]\n",
        "\n",
        "  return data\n",
        "\n",
        "xy_train_scale = HJ_MinMaxScaler(xy_train)\n",
        "xy_test_scale = HJ_MinMaxScaler(xy_test)\n",
        "xy_train_Y_scale = HJ_MinMaxScaler(xy_train_Y)\n",
        "xy_test_Y_scale = HJ_MinMaxScaler(xy_test_Y)\n",
        "\n",
        "def reshape_timeseries_to_image(data, window_size):\n",
        "    num_features = data.shape[1]\n",
        "    num_samples = data.shape[0] - window_size + 1\n",
        "    reshaped_data = np.zeros((num_samples, window_size, num_features))\n",
        "\n",
        "    for i in range(num_samples):\n",
        "        reshaped_data[i] = data[i:i+window_size]\n",
        "\n",
        "    return reshaped_data\n",
        "\n",
        "TIME_STEPS = 4\n",
        "\n",
        "train_image = reshape_timeseries_to_image(xy_train_scale, TIME_STEPS)\n",
        "test_image = reshape_timeseries_to_image(xy_test_scale, TIME_STEPS)\n",
        "train_Y_image = reshape_timeseries_to_image(xy_train_Y_scale, TIME_STEPS)\n",
        "test_Y_image = reshape_timeseries_to_image(xy_test_Y, TIME_STEPS)\n",
        "print(\"Train image with Lossing .shape : \", train_image.shape,train_image[0,:,:])\n",
        "print(\"Train image.shape : \", train_Y_image.shape,train_image[0,:,:])\n",
        "print(\"Test image with Loosing .shape : \" ,test_image.shape, test_image[0,:,:])\n",
        "print(\"Test image.shape : \" ,test_Y_image.shape, test_image[0,:,:])"
      ],
      "metadata": {
        "id": "m0TLGARkObgk"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import numpy as np\n",
        "import tensorflow as tf\n",
        "from tensorflow.keras.layers import Input, Conv2D\n",
        "from tensorflow.keras.models import Model\n",
        "from sklearn.metrics import mean_absolute_error\n",
        "\n",
        "# 이 부분에 실제 데이터 로드 코드를 추가하세요.\n",
        "# 예시: train_image = np.load('train_image.npy')\n",
        "# 예시: test_image = np.load('test_image.npy')\n",
        "\n",
        "# 원본 데이터 복사본 저장\n",
        "original_data = train_image.copy()\n",
        "\n",
        "# 각 시퀀스에 대해 N개의 결측치를 만듭니다.\n",
        "N = 2  # 한 시퀀스 내에서 0으로 만들 값의 수\n",
        "for i in range(train_image.shape[0]):\n",
        "    idx_missing = np.random.choice(train_image.shape[1]*train_image.shape[2], N, replace=False)\n",
        "    train_image[i, idx_missing // train_image.shape[2], idx_missing % train_image.shape[2]] = 0\n",
        "\n",
        "def data_generator(data, batch_size=32):\n",
        "    while True:\n",
        "        # Shuffle data\n",
        "        np.random.shuffle(data)\n",
        "\n",
        "        for i in range(0, len(data), batch_size):\n",
        "            data_batch = data[i : i + batch_size]\n",
        "\n",
        "            # 데이터 복사본 생성\n",
        "            data_with_missing = data_batch.copy()\n",
        "            data_interpolated = data_batch.copy()\n",
        "\n",
        "            # 결측치 생성 및 보간\n",
        "            for i in range(data_with_missing.shape[0]):\n",
        "                idx_missing = np.random.choice(data_with_missing.shape[1]*data_with_missing.shape[2], N, replace=False)\n",
        "                data_with_missing[i, idx_missing // data_with_missing.shape[2], idx_missing % data_with_missing.shape[2]] = 0\n",
        "                data_interpolated[i, idx_missing // data_with_missing.shape[2], idx_missing % data_with_missing.shape[2]] = np.mean(data_with_missing[i])\n",
        "\n",
        "            yield data_interpolated.reshape(data_with_missing.shape[0], data_with_missing.shape[1], data_with_missing.shape[2], 1), data_with_missing\n",
        "\n",
        "# 자동 혼합 정밀도 설정\n",
        "policy = tf.keras.mixed_precision.Policy('mixed_float16')\n",
        "tf.keras.mixed_precision.set_global_policy(policy)\n",
        "\n",
        "train_gen = data_generator(train_image)\n",
        "\n",
        "# 2D CNN 모델 정의\n",
        "inputs = Input(shape=(4, 4, 1))\n",
        "x = Conv2D(256, (3, 3), activation='relu', padding='same')(inputs)\n",
        "x = Conv2D(256, (3, 3), activation='relu', padding='same')(x)\n",
        "x = Conv2D(256, (2, 2), activation='relu', padding='same')(x)\n",
        "x = Conv2D(256, (2, 2), activation='relu', padding='same')(x)\n",
        "x = Conv2D(256, (2, 2), activation='relu', padding='same')(x)\n",
        "outputs = Conv2D(1, (2, 2), activation='linear', padding='same')(x)\n",
        "outputs = tf.keras.layers.Activation('linear', dtype='float32')(outputs)  # 마지막 레이어는 float32로 설정\n",
        "\n",
        "model = Model(inputs=inputs, outputs=outputs)\n",
        "\n",
        "# 모델 컴파일 및 훈련\n",
        "model.compile(optimizer='adam', loss='mse')\n",
        "model.fit(train_gen, steps_per_epoch=len(train_image) // 32, epochs=100)\n",
        "\n",
        "# 결측치 예측\n",
        "predicted = model.predict(test_image.reshape(test_image.shape[0], test_image.shape[1], test_image.shape[2], 1))\n",
        "\n",
        "# 0값을 예측값으로 채우기\n",
        "data_filled = test_image.copy()\n",
        "zero_indices = np.argwhere(test_image == 0)\n",
        "for ind in zero_indices:\n",
        "    data_filled[ind[0], ind[1], ind[2]] = predicted[ind[0], ind[1], ind[2]]\n",
        "\n",
        "# 복구 정확도 계산\n",
        "missing_indices = np.argwhere(test_image == 0)\n",
        "mae = mean_absolute_error(original_data[missing_indices[:, 0], missing_indices[:, 1], missing_indices[:, 2]], data_filled[missing_indices[:, 0], missing_indices[:, 1], missing_indices[:, 2]])\n",
        "print(\"복구 정확도 (MAE): \", mae)\n"
      ],
      "metadata": {
        "id": "2Cevq8x_Qlac"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import matplotlib.pyplot as plt\n",
        "\n",
        "# 예측하기 위해 결측치가 있는 데이터 준비\n",
        "idx = 0  # 시각화하려는 데이터 인덱스\n",
        "data_with_missing = test_image[idx].copy()\n",
        "idx_missing = np.random.choice(data_with_missing.shape[0]*data_with_missing.shape[1], N, replace=False)\n",
        "data_with_missing[idx_missing // data_with_missing.shape[1], idx_missing % data_with_missing.shape[1]] = 0\n",
        "\n",
        "# 결측치 예측\n",
        "predicted = model.predict(data_with_missing.reshape(1, data_with_missing.shape[0], data_with_missing.shape[1], 1))\n",
        "\n",
        "# 예측값으로 0 채우기\n",
        "data_filled = data_with_missing.copy()\n",
        "data_filled[data_with_missing == 0] = predicted[0, data_with_missing == 0]\n",
        "\n",
        "# 시각화\n",
        "fig, ax = plt.subplots(1, 3, figsize=(15, 5))\n",
        "\n",
        "# 원본 데이터 시각화\n",
        "ax[0].imshow(test_image[idx], cmap='gray')\n",
        "ax[0].set_title('Original Data')\n",
        "\n",
        "# 결측치가 있는 데이터 시각화\n",
        "ax[1].imshow(data_with_missing, cmap='gray')\n",
        "ax[1].set_title('Data with Missing Values')\n",
        "\n",
        "# 복구된 데이터 시각화\n",
        "ax[2].imshow(data_filled, cmap='gray')\n",
        "ax[2].set_title('Recovered Data')\n",
        "\n",
        "plt.show()\n"
      ],
      "metadata": {
        "id": "jGw4eMisZ67G"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}